{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsysC3sdZalD"
   },
   "source": [
    "# Homework\n",
    "\n",
    "1. จงสร้างโมเดล Logistic Regression ที่รับค่า Input เป็นคอลลัมดังต่อไปนี้ ['pH', 'density', 'alcohol'] โดยให้นำ Input ไป Scale ผ่าน Standard Scaler เเละ Output เป็นเกณฑ์ว่า 'quality' ของ ข้อมูลมากกว่า 5.0 หรือไม่ เเละเเสดงผลค่า Accuracy ที่ได้หลังจากฝึกเสร็จ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "executionInfo": {
     "elapsed": 3104,
     "status": "ok",
     "timestamp": 1732287878320,
     "user": {
      "displayName": "Jirayu Petchhan",
      "userId": "11874020435605929065"
     },
     "user_tz": -420
    },
    "id": "7x0YkMs5ZbzL",
    "outputId": "1c7ffeb7-4f80-4b3f-c3f1-7384ad40d6bf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>fixed.acidity</th>\n",
       "      <th>volatile.acidity</th>\n",
       "      <th>citric.acid</th>\n",
       "      <th>residual.sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free.sulfur.dioxide</th>\n",
       "      <th>total.sulfur.dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  fixed.acidity  volatile.acidity  citric.acid  residual.sugar  \\\n",
       "0           1            7.4              0.70         0.00             1.9   \n",
       "1           2            7.8              0.88         0.00             2.6   \n",
       "2           3            7.8              0.76         0.04             2.3   \n",
       "3           4           11.2              0.28         0.56             1.9   \n",
       "4           5            7.4              0.70         0.00             1.9   \n",
       "\n",
       "   chlorides  free.sulfur.dioxide  total.sulfur.dioxide  density    pH  \\\n",
       "0      0.076                 11.0                  34.0   0.9978  3.51   \n",
       "1      0.098                 25.0                  67.0   0.9968  3.20   \n",
       "2      0.092                 15.0                  54.0   0.9970  3.26   \n",
       "3      0.075                 17.0                  60.0   0.9980  3.16   \n",
       "4      0.076                 11.0                  34.0   0.9978  3.51   \n",
       "\n",
       "   sulphates  alcohol  quality  \n",
       "0       0.56      9.4        0  \n",
       "1       0.68      9.8        0  \n",
       "2       0.65      9.8        0  \n",
       "3       0.58      9.8        1  \n",
       "4       0.56      9.4        0  "
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\user\\git\\Machine-Learning-Lab-KMITL\\week_3 homework\\wineQualityReds.csv')\n",
    "df['quality'] = np.where(df['quality'] > 5.0, 1, 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.28864292,  0.55827446, -0.96024611],\n",
       "       [-0.7199333 ,  0.02826077, -0.58477711],\n",
       "       [-0.33117661,  0.13426351, -0.58477711],\n",
       "       ...,\n",
       "       [ 0.70550789, -0.53355375,  0.54162988],\n",
       "       [ 1.6773996 , -0.67665745, -0.20930812],\n",
       "       [ 0.51112954, -0.66605717,  0.54162988]])"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X =  df[['pH', 'density', 'alcohol']]\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y = df['quality']\n",
    "\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1599, 3) (1599, 1)\n",
      "Iteration 0, Cost: 8.570973398968457\n",
      "Iteration 100, Cost: 5.771582890394081\n",
      "Iteration 200, Cost: 6.289988540130077\n",
      "Iteration 300, Cost: 6.889035068713895\n",
      "Iteration 400, Cost: 6.197827535732568\n",
      "Iteration 500, Cost: 8.087128125881527\n",
      "Iteration 600, Cost: 10.586995370163997\n",
      "Iteration 700, Cost: 8.248409883577171\n",
      "Iteration 800, Cost: 7.3959205929002\n",
      "Iteration 900, Cost: 5.852223769241903\n"
     ]
    }
   ],
   "source": [
    "# @title LogReg from scratch (BCE)\n",
    "class LogisticRegressionScratchBCE:\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def predict(self, X):\n",
    "        z = np.dot(X, self.w) + self.b\n",
    "        probs = self.sigmoid(z)\n",
    "\n",
    "        # Return the class with the highest probability\n",
    "        return np.where(probs >= 0.5, 1, 0)\n",
    "\n",
    "    def initialize_weights(self, n_features):\n",
    "        self.w = np.zeros((n_features, 1))  # Init with the same column number as feature\n",
    "        self.b = 0\n",
    "\n",
    "    def cost_function(self, h, y):\n",
    "        m = len(y)\n",
    "        # reg_term = (0.01 / (2 * m)) * np.sum(self.w ** 2)\n",
    "        cost = -(1 / m) * np.sum(y * np.log(h + 1e-8) + (1 - y) * np.log(1 - h + 1e-8))\n",
    "\n",
    "        return cost #+ reg_term\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y).reshape(-1, 1)  # Ensure y is a column vector\n",
    "        print(X.shape, y.shape)\n",
    "        m = len(y)\n",
    "        n_features = X.shape[1]\n",
    "        self.initialize_weights(n_features)\n",
    "\n",
    "        for i in range(self.num_iterations):\n",
    "            # Forward prop\n",
    "            probs = self.predict(X)\n",
    "\n",
    "            # Cost\n",
    "            # error = -(1 / m) * np.sum(y * np.log(probs + 1e-8) + (1 - y) * np.log(1 - probs + 1e-8))\n",
    "            error = self.cost_function(probs, y)\n",
    "\n",
    "            # Calculate the gradient of the error with respect to the weights\n",
    "            gradient_w = (1 / m) * np.dot(X.T, (probs - y))\n",
    "            gradient_b = (1 / m) * np.sum(probs - y)\n",
    "\n",
    "            # Update the weights using the gradient and the learning rate\n",
    "            self.w -= self.learning_rate * gradient_w\n",
    "            self.b -= self.learning_rate * gradient_b\n",
    "\n",
    "            # cost compute if more iteration (optional)\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Iteration {i}, Cost: {error}\")\n",
    "\n",
    "\n",
    "# Train Logistic Regression from scratch\n",
    "BCEmodel_scratch = LogisticRegressionScratchBCE(learning_rate=1e-4, num_iterations=1000)\n",
    "\n",
    "BCEmodel_scratch.fit(X_scaled, y)\n",
    "\n",
    "# Predict and evaluate\n",
    "BCE_pred_scratch = BCEmodel_scratch.predict(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7035647279549718"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = accuracy_score(y, BCE_pred_scratch)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
